# -*- coding: utf-8 -*-
"""Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ENT76Tm3uBtS9qgCcRfYzj6McOeB7xUI
"""

import pandas as pd

# Load the dataset
file_path = "C:/Users/Jamil Shaikh/Desktop/HR_comma_sep.csv"
data =pd.read_csv(file_path)

# Correct the column names
data.rename(columns={
    'average_montly_hours': 'average_monthly_hours',
    'time_spend_company': 'time_spent_company',
    'Department': 'department'
}, inplace=True)

# Define features and target
X = data.drop(columns=['satisfaction_level'])
y = data['satisfaction_level']

# Define categorical and numerical columns
categorical_cols = ['department', 'salary']
numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
numerical_cols = [col for col in numerical_cols if col not in categorical_cols]

# Feature Engineering: Creating new feature
X['average_monthly_hours_per_year'] = X['average_monthly_hours'] * 12

# Update numerical columns to include the new feature
numerical_cols.append('average_monthly_hours_per_year')

import seaborn as sns
import matplotlib.pyplot as plt

# Determine the number of rows needed based on the number of numerical columns
num_cols = 4  # Number of figures per row
num_rows = (len(numerical_cols) + num_cols - 1) // num_cols  # Calculate the number of rows

fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 5))

# Flatten axes array if there are multiple rows
axes = axes.flatten()

for i, col in enumerate(numerical_cols):
    sns.histplot(X[col], kde=True, bins=30, ax=axes[i])
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Frequency')

# Remove any empty subplots if there are fewer columns than grid slots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Visualizing categorical features
for col in categorical_cols:
    plt.figure(figsize=(10, 6))
    sns.countplot(x=col, data=X)
    plt.title(f'Count of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()

# Visualizing correlation matrix for numerical features
plt.figure(figsize=(12, 8))
corr_matrix = X[numerical_cols].corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix for Numerical Features')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import numpy as np
import matplotlib.pyplot as plt



# Create new features for Model 2
data['average_hours_per_project'] = data['average_monthly_hours'] / data['number_project']
data['high_satisfaction'] = (data['satisfaction_level'] > 0.7).astype(int)

# Define features for Model 1
features_model_1 = ['last_evaluation', 'number_project', 'average_monthly_hours', 'time_spent_company',
                    'Work_accident', 'left', 'promotion_last_5years', 'department', 'salary']
X1 = data[features_model_1]
y = data['satisfaction_level']

# Define features for Model 2
features_model_2 = ['last_evaluation', 'number_project', 'average_monthly_hours', 'time_spent_company',
                    'Work_accident', 'left', 'promotion_last_5years', 'department', 'salary',
                    'average_hours_per_project', 'high_satisfaction']
X2 = data[features_model_2]

# Function to preprocess data and train model
def train_model(X, y, model_filename):
    # Split the data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Define numerical and categorical columns
    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    print(f"Numerical columns: {numerical_cols}")
    print(f"Categorical columns: {categorical_cols}")

    # Preprocessing for numerical data
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    # Preprocessing for categorical data
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Bundle preprocessing for numerical and categorical data
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])

    # Define the model
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', RandomForestRegressor(random_state=42))
    ])

    # Define parameter grid for GridSearchCV
    param_grid = {
        'regressor__n_estimators': [100, 200],
        'regressor__max_features': ['auto', 'sqrt', 'log2'],
        'regressor__max_depth': [None, 10, 20, 30]
    }

    # Perform GridSearch to find the best parameters
    grid_search = GridSearchCV(model_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Best model
    best_model = grid_search.best_estimator_

    # Make predictions with the best model
    y_pred = best_model.predict(X_test)

    # Evaluate the best model
    mse = mean_squared_error(y_test, y_pred)
    rmse = mean_squared_error(y_test, y_pred, squared=False)  # Calculate RMSE by setting squared=False
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    adjusted_r2 = 1 - (1 - r2) * (len(y_test) - 1) / (len(y_test) - X_test.shape[1] - 1)
    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # Calculate MAPE

    # Display the evaluation metrics
    evaluation_results = {
        'Mean Squared Error': mse,
        'Root Mean Squared Error': rmse,
        'Mean Absolute Error': mae,
        'R-squared': r2,
        'Adjusted R-squared': adjusted_r2,
        'Mean Absolute Percentage Error': mape
    }

    print(f"Evaluation Results for {model_filename}:", evaluation_results)

    # Save the best model
    joblib.dump(best_model, model_filename)
    print(f"Model saved to {model_filename}")

    # Feature importance
    importances = best_model.named_steps['regressor'].feature_importances_

    # Ensure the feature names are correct
    preprocessor = best_model.named_steps['preprocessor']
    onehot_feature_names = []
    if 'cat' in dict(preprocessor.named_transformers_).keys():
        onehot_feature_names = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_cols).tolist()

    feature_names = numerical_cols + onehot_feature_names
    print(f"Feature names: {feature_names}")
    print(f"Importances shape: {importances.shape}")
    print(f"Feature names shape: {len(feature_names)}")

    # Plot feature importance
    plt.figure(figsize=(12, 8))
    plt.barh(feature_names, importances)
    plt.xlabel('Feature Importance')
    plt.ylabel('Feature')
    plt.title(f'Feature Importance in Predicting Employee Satisfaction ({model_filename})')
    plt.show()

# Train and save Model 1
train_model(X1, y, 'model_1.pkl')

# Train and save Model 2
train_model(X2, y, 'model_2.pkl')
